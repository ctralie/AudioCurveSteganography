{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy import signal\n",
    "import glob\n",
    "import librosa\n",
    "import time\n",
    "import pickle\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import skimage.io\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from imtools import get_voronoi_image, splat_voronoi_image_1nn\n",
    "from tsp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5379f7",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0831bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_time(X, hop_length, mode='nearest'):\n",
    "    \"\"\"\n",
    "    Upsample a tensor by a factor of hop_length along the time axis\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: torch.tensor(M, T, N)\n",
    "        A tensor in which the time axis is axis 1\n",
    "    hop_length: int\n",
    "        Upsample factor\n",
    "    mode: string\n",
    "        Mode of interpolation.  'nearest' by default to avoid artifacts\n",
    "        where notes in the violin jump by large intervals\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.tensor(M, T*hop_length, N)\n",
    "        Upsampled tensor\n",
    "    \"\"\"\n",
    "    X = X.permute(0, 2, 1)\n",
    "    X = nn.functional.interpolate(X, size=hop_length*X.shape[-1], mode=mode)\n",
    "    return X.permute(0, 2, 1)\n",
    "\n",
    "################################################\n",
    "# Loudness code modified from original Google Magenta DDSP implementation in tensorflow\n",
    "# https://github.com/magenta/ddsp/blob/86c7a35f4f2ecf2e9bb45ee7094732b1afcebecd/ddsp/spectral_ops.py#L253\n",
    "# which, like this repository, is licensed under Apache2 by Google Magenta Group, 2020\n",
    "# Modifications by Chris Tralie, 2023\n",
    "\n",
    "def power_to_db(power, ref_db=0.0, range_db=80.0, use_tf=True):\n",
    "    \"\"\"Converts power from linear scale to decibels.\"\"\"\n",
    "    # Convert to decibels.\n",
    "    db = 10.0*np.log10(np.maximum(power, 10**(-range_db/10)))\n",
    "    # Set dynamic range.\n",
    "    db -= ref_db\n",
    "    db = np.maximum(db, -range_db)\n",
    "    return db\n",
    "\n",
    "def extract_loudness(x, sr, hop_length, n_fft=512):\n",
    "    \"\"\"\n",
    "    Extract the loudness in dB by using an A-weighting of the power spectrum\n",
    "    (section B.1 of the paper)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray(N)\n",
    "        Audio samples\n",
    "    sr: int\n",
    "        Sample rate (used to figure out frequencies for A-weighting)\n",
    "    hop_length: int\n",
    "        Hop length between loudness estimates\n",
    "    n_fft: int\n",
    "        Number of samples to use in each window\n",
    "    \"\"\"\n",
    "    # Computed centered STFT\n",
    "    S = librosa.stft(x, n_fft=n_fft, hop_length=hop_length, win_length=n_fft, center=True)\n",
    "    \n",
    "    # Compute power spectrogram\n",
    "    amplitude = np.abs(S)\n",
    "    power = amplitude**2\n",
    "\n",
    "    # Perceptual weighting.\n",
    "    freqs = np.arange(S.shape[0])*sr/n_fft\n",
    "    a_weighting = librosa.A_weighting(freqs)[:, None]\n",
    "\n",
    "    # Perform weighting in linear scale, a_weighting given in decibels.\n",
    "    weighting = 10**(a_weighting/10)\n",
    "    power = power * weighting\n",
    "\n",
    "    # Average over frequencies (weighted power per a bin).\n",
    "    avg_power = np.mean(power, axis=0)\n",
    "    loudness = power_to_db(avg_power)\n",
    "    return np.array(loudness, dtype=np.float32)\n",
    "\n",
    "################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d73838",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb47884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurveData(Dataset):\n",
    "    def __init__(self, rg, voronoi_samples, T, samples_per_batch):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        rg: list(int)\n",
    "            List of indices to take in each image class (used for test/train split)\n",
    "        voronoi_samples: int\n",
    "            Number of samples in the Voronoi image\n",
    "        T: int\n",
    "            Number of samples to take in each chunk\n",
    "        samples_per_batch: int\n",
    "            Number of samples per batch\n",
    "        \"\"\"\n",
    "        self.files = []\n",
    "        for c in glob.glob(\"../data/imagenet/*\"): # Go through each class\n",
    "            files = glob.glob(\"{}/*.pkl\".format(c))\n",
    "            files = sorted(files)\n",
    "            self.files += [files[i] for i in rg]\n",
    "        # Load in all curves\n",
    "        self.Ys = []\n",
    "        for file in self.files:\n",
    "            res = pickle.load(open(file, \"rb\"))\n",
    "            Y = res[voronoi_samples][\"Y\"]\n",
    "            if Y.shape[0] >= T:\n",
    "                self.Ys.append(np.array(Y, dtype=np.float32))\n",
    "        self.T = T\n",
    "        self.samples_per_batch = samples_per_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.samples_per_batch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Pull out a random chunk of the appropriate length from a random file\n",
    "        \"\"\"\n",
    "        idx = np.random.randint(len(self.Ys))\n",
    "        Y = self.Ys[idx]\n",
    "        Y = np.roll(Y, np.random.randint(Y.shape[0]), axis=0)\n",
    "        Y = Y[0:self.T, :]\n",
    "        return torch.from_numpy(Y)\n",
    "        \n",
    "class AudioData(Dataset):\n",
    "    def __init__(self, file_pattern, T, samples_per_batch, win_length, sr):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_pattern: string\n",
    "            File pattern to match for audio files\n",
    "        T: int\n",
    "            Number of windows to take in each chunk\n",
    "        samples_per_batch: int\n",
    "            Number of samples per batch\n",
    "        win_length: int\n",
    "            Window length of STFT; hop_length assumed to be half of this\n",
    "        sr: int\n",
    "            Sample rate\n",
    "        \"\"\"\n",
    "        self.samples_per_batch = samples_per_batch\n",
    "        hop_length = win_length//2\n",
    "        self.hop_length = hop_length\n",
    "        self.T = T\n",
    "        self.n_samples = hop_length*(T-1)+win_length\n",
    "        self.samples = []\n",
    "        self.loudnesses = []\n",
    "        for filename in glob.glob(file_pattern):\n",
    "            x, _ = librosa.load(filename, sr=sr)\n",
    "            loudness = extract_loudness(x, sr, hop_length, n_fft=win_length)\n",
    "            x = np.array(x, dtype=np.float32)\n",
    "            loudness = np.array(loudness, dtype=np.float32)\n",
    "            self.samples.append(x)\n",
    "            self.loudnesses.append(loudness)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples_per_batch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a random audio clip, along with its loudness\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor(n_samples)\n",
    "            Audio clip\n",
    "        torch.tensor(T, 1)\n",
    "            Loudness\n",
    "        \"\"\"\n",
    "        idx = np.random.randint(len(self.samples))\n",
    "        x = self.samples[idx]\n",
    "        loudness = self.loudnesses[idx]\n",
    "        i1 = np.random.randint(len(loudness)-T-1)\n",
    "        loudness = loudness[i1:i1+self.T]\n",
    "        loudness = loudness[:, None]\n",
    "        i1 = i1*self.hop_length\n",
    "        x = x[i1:i1+self.n_samples]\n",
    "        return torch.from_numpy(x), torch.from_numpy(loudness)\n",
    "        \n",
    "        \n",
    "n_samples = 3000\n",
    "T = 300\n",
    "samples_per_batch = 10000\n",
    "win_length = 1024\n",
    "sr = 44100\n",
    "\n",
    "curve_train = CurveData(np.arange(10), n_samples, T, samples_per_batch)\n",
    "curve_test  = CurveData(np.arange(10, 15), n_samples, T, samples_per_batch)\n",
    "\n",
    "audio_train = AudioData(\"../data/musdb18hq/train/*/mixture.wav\", T, samples_per_batch, win_length, sr)\n",
    "audio_test  = AudioData(\"../data/musdb18hq/test/*/mixture.wav\",  T, samples_per_batch, win_length, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff5a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(curve_train, batch_size=16, shuffle=True)\n",
    "Y = next(iter(loader)).numpy()\n",
    "print(Y.shape)\n",
    "plt.scatter(Y[0, :, 0], Y[0, :, 1], c=Y[0, :, 2::])\n",
    "plt.plot(Y[0, :, 0], Y[0, :, 1], c='k', linewidth=1)\n",
    "\n",
    "loader = DataLoader(audio_train, batch_size=16, shuffle=True)\n",
    "X, L = next(iter(loader))\n",
    "ipd.Audio(X[0, :], rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b5830",
   "metadata": {},
   "source": [
    "# Encoder / Decoder Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed214aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, depth=3, n_input=1, n_units=512):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(n_input, n_units))\n",
    "            else:\n",
    "                layers.append(nn.Linear(n_units, n_units))\n",
    "            layers.append(nn.LayerNorm(normalized_shape=n_units))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        total = 0\n",
    "        for p in self.parameters():\n",
    "            total += np.prod(p.shape)\n",
    "        return total\n",
    "            \n",
    "def modified_sigmoid(x):\n",
    "    return 2*torch.sigmoid(x)**np.log(10) + 1e-7\n",
    "        \n",
    "\n",
    "def get_filtered_noise(H, A, win_length):\n",
    "    \"\"\"\n",
    "    Perform subtractive synthesis by applying FIR filters to windows\n",
    "    and summing overlap-added versions of them together\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    H: torch.tensor(n_batches x time x n_coeffs)\n",
    "        FIR filters for each window for each batch\n",
    "    A: torch.tensor(n_batches x time x 1)\n",
    "        Amplitudes for each window for each batch\n",
    "    win_length: int\n",
    "        Window length of each chunk to which to apply FIR filter.\n",
    "        Hop length is assumed to be half of this\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.tensor(n_batches, hop_length*(time-1)+win_length)\n",
    "        Filtered noise for each batch\n",
    "    \"\"\"\n",
    "    n_batches = H.shape[0]\n",
    "    T = H.shape[1]\n",
    "    n_coeffs = H.shape[2]\n",
    "    hop_length = win_length//2\n",
    "    n_samples = hop_length*(T-1)+win_length\n",
    "\n",
    "    ## Pad impulse responses and generate noise\n",
    "    H = nn.functional.pad(H, (0, win_length*2-n_coeffs))\n",
    "    noise = torch.randn(n_batches, n_samples).to(H)\n",
    "\n",
    "    ## Take out each overlapping window of noise\n",
    "    N = torch.zeros(n_batches, T, win_length*2).to(H)\n",
    "    n_even = n_samples//win_length\n",
    "    N[:, 0::2, 0:win_length] = noise[:, 0:n_even*win_length].view(n_batches, n_even, win_length)\n",
    "    n_odd = T - n_even\n",
    "    N[:, 1::2, 0:win_length] = noise[:, hop_length:hop_length+n_odd*win_length].view(n_batches, n_odd, win_length)\n",
    "    \n",
    "    # Apply amplitude to each window\n",
    "    N = N*A\n",
    "    \n",
    "    ## Perform a zero-phase version of each filter and window\n",
    "    FH = torch.fft.rfft(H)\n",
    "    FH = torch.real(FH)**2 + torch.imag(FH)**2 # Make it zero-phase\n",
    "    FN = torch.fft.rfft(N)\n",
    "    y = torch.fft.irfft(FH*FN)[..., 0:win_length]\n",
    "    y = y*torch.hann_window(win_length).to(y)\n",
    "\n",
    "    ## Overlap-add everything\n",
    "    ola = torch.zeros(n_batches, n_samples).to(y)\n",
    "    ola[:, 0:n_even*win_length] += y[:, 0::2, :].reshape(n_batches, n_even*win_length)\n",
    "    ola[:, hop_length:hop_length+n_odd*win_length] += y[:, 1::2, :].reshape(n_batches, n_odd*win_length)\n",
    "    \n",
    "    return ola\n",
    "    \n",
    "    \n",
    "        \n",
    "class CurveEncoder(nn.Module):\n",
    "    def __init__(self, mlp_depth, n_units, n_taps, win_length, pre_scale=0.01):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        mlp_depth: int\n",
    "            Depth of each multilayer perceptron\n",
    "        n_units: int\n",
    "            Number of units in each multilayer perceptron\n",
    "        n_taps: int\n",
    "            Number of taps in each FIR filter\n",
    "        win_length: int\n",
    "            Length of window for each windowed audio chunk\n",
    "        pre_scale: float\n",
    "            Initial ampitude of noise (try to start off much lower than audio)\n",
    "        \"\"\"\n",
    "        super(CurveEncoder, self).__init__()\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = win_length//2\n",
    "        self.pre_scale = pre_scale\n",
    "        \n",
    "        self.YMLP = MLP(mlp_depth, 5, n_units) # Curve MLP\n",
    "        self.LMLP = MLP(mlp_depth, 1, n_units) # Loudness MLP\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=n_units*2, hidden_size=n_units, num_layers=1, bias=True, batch_first=True)\n",
    "        self.FinalMLP = MLP(mlp_depth, n_units*3, n_units)\n",
    "        self.TapsDecoder = nn.Linear(n_units, n_taps)\n",
    "        self.AmplitudeDecoder = nn.Linear(n_units, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, Y, L):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y: torch.tensor(n_batches, T, 5)\n",
    "            xyrgb samples\n",
    "        L: torch.tensor(n_batches, T, 1)\n",
    "            \n",
    "        \"\"\"\n",
    "        YOut = self.YMLP(Y)\n",
    "        LOut = self.LMLP(L)\n",
    "        YL = torch.concatenate((YOut, LOut), axis=2)\n",
    "        G = self.gru(YL)[0]\n",
    "        G = torch.concatenate((YOut, LOut, G), axis=2)\n",
    "        final = self.FinalMLP(G)\n",
    "        H = nn.functional.tanh(self.TapsDecoder(final))\n",
    "        A = nn.functional.leaky_relu(self.AmplitudeDecoder(final))\n",
    "        N = get_filtered_noise(H, A, self.win_length)\n",
    "        return self.pre_scale*N\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        total = 0\n",
    "        for p in self.parameters():\n",
    "            total += np.prod(p.shape)\n",
    "        return total\n",
    "        \n",
    "\n",
    "        \n",
    "class CurveDecoder(nn.Module):\n",
    "    def __init__(self, mlp_depth, n_units, win_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        mlp_depth: int\n",
    "            Depth of each multilayer perceptron\n",
    "        n_units: int\n",
    "            Number of units in each multilayer perceptron\n",
    "        win_length: int\n",
    "            Length of window for each windowed audio chunk\n",
    "        \"\"\"\n",
    "        super(CurveDecoder, self).__init__()\n",
    "        self.win_length = win_length\n",
    "        \n",
    "        self.SMLP = MLP(mlp_depth, win_length//2+1, n_units) # STFT MLP\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=n_units, hidden_size=n_units, num_layers=1, bias=True, batch_first=True)\n",
    "        self.FinalMLP = MLP(mlp_depth, n_units*2, n_units)\n",
    "        self.YDecoder = nn.Linear(n_units, 5)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y: torch.tensor(n_batches, n_samples)\n",
    "            Audio samples\n",
    "        \"\"\"\n",
    "        win = self.win_length\n",
    "        hop = win//2\n",
    "        hann = torch.hann_window(win).to(X)\n",
    "        S = torch.abs(torch.stft(X, win, hop, win, hann, return_complex=True, center=False))\n",
    "        S = torch.swapaxes(S, 1, 2)\n",
    "        SOut = self.SMLP(S)\n",
    "        G = self.gru(SOut)[0]\n",
    "        G = torch.concatenate((SOut, G), axis=2)\n",
    "        final = self.FinalMLP(G)\n",
    "        return modified_sigmoid(self.YDecoder(final))\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        total = 0\n",
    "        for p in self.parameters():\n",
    "            total += np.prod(p.shape)\n",
    "        return total\n",
    "\n",
    "\n",
    "curve_loader = DataLoader(curve_train, batch_size=16, shuffle=True)\n",
    "audio_loader = DataLoader(audio_train, batch_size=16, shuffle=True)\n",
    "Y = next(iter(curve_loader))\n",
    "X, L = next(iter(audio_loader))\n",
    "\n",
    "mlp_depth = 3\n",
    "n_units = 256\n",
    "n_taps = 50\n",
    "encoder = CurveEncoder(mlp_depth, n_units, n_taps, win_length)\n",
    "decoder = CurveDecoder(mlp_depth, n_units, win_length)\n",
    "print(\"Encoder params\", encoder.get_num_parameters())\n",
    "print(\"Decoder params\", decoder.get_num_parameters())\n",
    "N = encoder(Y, L)\n",
    "ipd.Audio(N.detach().numpy()[1, :], rate=sr)\n",
    "XN = X + N\n",
    "YOut = decoder(XN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6df390",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc720c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HANN_TABLE = {}\n",
    "\n",
    "def mss_loss(X, Y, eps=1e-7):\n",
    "    loss = 0\n",
    "    win = 64\n",
    "    while win <= 2048:\n",
    "        hop = win//4\n",
    "        if not win in HANN_TABLE:\n",
    "            HANN_TABLE[win] = torch.hann_window(win).to(X)\n",
    "        hann = HANN_TABLE[win]\n",
    "        SX = torch.abs(torch.stft(X.squeeze(), win, hop, win, hann, return_complex=True))\n",
    "        SY = torch.abs(torch.stft(Y.squeeze(), win, hop, win, hann, return_complex=True))\n",
    "        loss_win = torch.sum(torch.abs(SX-SY)) + torch.sum(torch.abs(torch.log(SX+eps)-torch.log(SY+eps)))\n",
    "        loss += loss_win/torch.numel(SX)\n",
    "        win *= 2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5adf2b",
   "metadata": {},
   "source": [
    "# Test Example - Layla And Smooth Criminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48484c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "n_points = 3000\n",
    "n_iters = 100\n",
    "I = skimage.io.imread(\"../data/images/layla.png\")\n",
    "N = min(I.shape[0], I.shape[1])\n",
    "J, YLayla, final_cost = get_voronoi_image(I, device, n_points, n_neighbs=2, n_iters=n_iters, verbose=False, plot_iter_interval=0, use_lsqr=False)\n",
    "YLayla = get_tsp_tour(YLayla)\n",
    "#YLayla = curve_train.Ys[100]\n",
    "plt.imshow(splat_voronoi_image_1nn(YLayla, 200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9750be",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "YLayla = np.array(YLayla, dtype=np.float32)\n",
    "hop_length = win_length//2\n",
    "TLayla = YLayla.shape[0]\n",
    "YLayla = torch.from_numpy(YLayla).to(device)\n",
    "YLayla = YLayla.view(1, TLayla, 5)\n",
    "xsmooth, _ = librosa.load(\"../data/tunes/smoothcriminal.mp3\", sr=sr)\n",
    "#xsmooth, _ = librosa.load(\"../data/musdb18hq/test/Arise - Run Run Run/mixture.wav\", sr=sr)\n",
    "xsmooth = xsmooth[sr*15::] # Cut off quieter beginning\n",
    "lsmooth = extract_loudness(xsmooth, sr, hop_length, win_length)\n",
    "lsmooth = np.array(lsmooth[0:TLayla], dtype=np.float32)\n",
    "xsmooth = np.array(xsmooth[0:hop_length*(TLayla-1)+win_length], dtype=np.float32)\n",
    "xsmooth = torch.from_numpy(xsmooth[None, :]).to(device)\n",
    "lsmooth = torch.from_numpy(lsmooth[None, :, None]).to(device)\n",
    "print(xsmooth.shape, lsmooth.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7deb662",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fbcf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print(\"Device: \", device)\n",
    "\n",
    "## Step 2: Create model with a test batch\n",
    "noise_only = False\n",
    "mlp_depth = 3\n",
    "n_units = 512\n",
    "n_taps = 200\n",
    "pre_scale = 1\n",
    "lam_xy = 1000 # Weight for geometry curve fit\n",
    "lam_rgb = 200 # Weight for rgb curve fit\n",
    "encoder = CurveEncoder(mlp_depth, n_units, n_taps, win_length, pre_scale)\n",
    "encoder = encoder.to(device)\n",
    "decoder = CurveDecoder(mlp_depth, n_units, win_length)\n",
    "decoder = decoder.to(device)\n",
    "print(\"Encoder params\", encoder.get_num_parameters())\n",
    "print(\"Decoder params\", decoder.get_num_parameters())\n",
    "\n",
    "## Step 3: Setup the loss function\n",
    "batch_size=16\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)\n",
    "# Start at learning rate of 0.001, rate decay of 0.98 factor every 10,000 steps\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000/len(curve_train), gamma=0.98)\n",
    "\n",
    "n_epochs = 10000\n",
    "xy_losses = []\n",
    "rgb_losses = []\n",
    "audio_losses = []\n",
    "\n",
    "test_xy_losses = []\n",
    "test_rgb_losses = []\n",
    "test_audio_losses = []\n",
    "\n",
    "fac = 0.7\n",
    "plt.figure(figsize=(fac*18, fac*18))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    #####################    STEP 1:  TRAIN     ##################### \n",
    "    curve_loader = DataLoader(curve_train, batch_size=16, shuffle=True)\n",
    "    audio_loader = DataLoader(audio_train, batch_size=16, shuffle=True)\n",
    "    \n",
    "    train_xy_loss = 0\n",
    "    train_rgb_loss = 0\n",
    "    train_audio_loss = 0\n",
    "    for batch_num, (Y, (X, L)) in enumerate(zip(curve_loader, audio_loader)): # Go through each mini batch\n",
    "        # Move inputs/outputs to GPU\n",
    "        Y = Y.to(device)\n",
    "        X = X.to(device)\n",
    "        L = L.to(device)\n",
    "        # Reset the optimizer's gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Encode the curve\n",
    "        N = encoder(Y, L)\n",
    "        # Add filtered noise to audio and decode\n",
    "        if noise_only:\n",
    "            XN = N\n",
    "        else:\n",
    "            XN = X + N\n",
    "        YOut = decoder(XN)\n",
    "        \n",
    "        # Add a loss term for the MSS fit of XN to X, as well as the L1 fit of Y to YOut\n",
    "        loss_audio = mss_loss(X, XN)\n",
    "        loss_xy = torch.mean(torch.abs(Y[:, :, 0:2]-YOut[:, :, 0:2]))\n",
    "        loss_rgb = torch.mean(torch.abs(Y[:, :, 2::]-YOut[:, :, 2::]))\n",
    "        \n",
    "        loss = lam_xy*loss_xy + lam_rgb*loss_rgb + loss_audio\n",
    "        \n",
    "        train_xy_loss += loss_xy.item()\n",
    "        train_rgb_loss += loss_rgb.item()\n",
    "        train_audio_loss += loss_audio.item()\n",
    "        \n",
    "        # Compute the gradients of the loss function with respect\n",
    "        # to all of the parameters of the model\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    last_Y = Y\n",
    "    last_YOut = YOut\n",
    "        \n",
    "    audio_losses.append(train_audio_loss/len(audio_loader))\n",
    "    xy_losses.append(train_xy_loss/len(audio_loader))\n",
    "    rgb_losses.append(train_rgb_loss/len(audio_loader))\n",
    "    \n",
    "    print(\"Epoch {}, audio loss {:.3f}, xy loss {:.3f}, rgb loss {:.3f}\".format(epoch, audio_losses[-1], xy_losses[-1], rgb_losses[-1]))\n",
    "    scheduler.step()\n",
    "    \n",
    "    #####################    STEP 2:  TEST     ##################### \n",
    "    curve_loader = DataLoader(curve_test, batch_size=16)\n",
    "    audio_loader = DataLoader(audio_test, batch_size=16)\n",
    "    test_xy_loss = 0\n",
    "    test_rgb_loss = 0\n",
    "    test_audio_loss = 0\n",
    "    for batch_num, (Y, (X, L)) in enumerate(zip(curve_loader, audio_loader)): # Go through each mini batch\n",
    "        # Move inputs/outputs to GPU\n",
    "        Y = Y.to(device)\n",
    "        X = X.to(device)\n",
    "        L = L.to(device)\n",
    "        # Encode the curve\n",
    "        N = encoder(Y, L)\n",
    "        # Add filtered noise to audio and decode\n",
    "        if noise_only:\n",
    "            XN = N\n",
    "        else:\n",
    "            XN = X + N\n",
    "        YOut = decoder(XN)\n",
    "        # Add a loss term for the MSS fit of XN to X, as well as the L1 fit of Y to YOut\n",
    "        loss_audio = mss_loss(X, XN)\n",
    "        loss_xy = torch.mean(torch.abs(Y[:, :, 0:2]-YOut[:, :, 0:2]))\n",
    "        loss_rgb = torch.mean(torch.abs(Y[:, :, 2::]-YOut[:, :, 2::]))\n",
    "        test_xy_loss += loss_xy.item()\n",
    "        test_rgb_loss += loss_rgb.item()\n",
    "        test_audio_loss += loss_audio.item()\n",
    "        \n",
    "    last_Y_test = Y\n",
    "    last_YOut_test = YOut\n",
    "        \n",
    "    test_audio_losses.append(test_audio_loss/len(audio_loader))\n",
    "    test_xy_losses.append(test_xy_loss/len(audio_loader))\n",
    "    test_rgb_losses.append(test_rgb_loss/len(audio_loader))\n",
    "    \n",
    "    \n",
    "    #####################    STEP 3:  LAYLA     ##################### \n",
    "    N = encoder(YLayla, lsmooth)\n",
    "    if noise_only:\n",
    "        XN = N\n",
    "    else:\n",
    "        XN = xsmooth + N\n",
    "    YOut = decoder(XN)\n",
    "    loss_xy_layla  = torch.mean(torch.abs(YLayla[:, :, 0:2]-YOut[:, :, 0:2]))\n",
    "    loss_rgb_layla = torch.mean(torch.abs(YLayla[:, :, 2::]-YOut[:, :, 2::]))\n",
    "    YOut = YOut.detach().cpu().numpy()\n",
    "    YOut = YOut[0, :, :]\n",
    "    YOut[:, 2::] = np.maximum(YOut[:, 2::], 0)\n",
    "    YOut[:, 2::] = np.minimum(YOut[:, 2::], 1)\n",
    "    \n",
    "    \n",
    "    #####################    STEP 4: PLOT    ##################### \n",
    "    plt.clf()\n",
    "    plt.subplot(331)\n",
    "    plt.scatter(YOut[:, 0], YOut[:, 1], c=YOut[:, 2::])\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.subplot(332)\n",
    "    plt.imshow(splat_voronoi_image_1nn(YOut, 200, 200))\n",
    "    \n",
    "    plt.subplot(333)\n",
    "    plt.plot(audio_losses)\n",
    "    plt.plot(lam_xy*np.array(xy_losses))\n",
    "    plt.plot(lam_rgb*np.array(rgb_losses))\n",
    "    plt.legend([\"Audio ({:.3f})\".format(audio_losses[-1]), \n",
    "                \"xy ({:.3f})\".format(xy_losses[-1]),\n",
    "                \"rgb ({:.3f})\".format(rgb_losses[-1])])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Scaled Loss\")\n",
    "    plt.title(\"Epoch {}, Train Losses\".format(epoch))\n",
    "    \n",
    "    plt.subplot(334)\n",
    "    x1 = last_Y.detach().cpu()[0, :, 0].numpy()\n",
    "    x2 = last_YOut.detach().cpu()[0, :, 0].numpy()\n",
    "    plt.scatter(x1, x2)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.title(\"X Coord Train (Mean {:.3f})\".format(np.mean(np.abs(x1-x2))))\n",
    "    plt.subplot(335)\n",
    "    x1 = last_Y.detach().cpu()[0, :, 2].numpy()\n",
    "    x2 = last_YOut.detach().cpu()[0, :, 2].numpy()\n",
    "    plt.scatter(x1, x2)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.title(\"R Coord Train (Mean {:.3f})\".format(np.mean(np.abs(x1-x2))))    \n",
    "    \n",
    "    plt.subplot(336)\n",
    "    plt.plot(test_audio_losses)\n",
    "    plt.plot(lam_xy*np.array(test_xy_losses))\n",
    "    plt.plot(lam_rgb*np.array(test_rgb_losses))\n",
    "    plt.legend([\"Audio ({:.3f})\".format(test_audio_losses[-1]), \n",
    "                \"xy ({:.3f})\".format(test_xy_losses[-1]),\n",
    "                \"rgb ({:.3f})\".format(test_rgb_losses[-1])])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Scaled Loss\")\n",
    "    plt.title(\"Test Losses\")\n",
    "    \n",
    "    \n",
    "    plt.subplot(337)\n",
    "    plt.scatter(YLayla.detach().cpu()[0, :, 0], YOut[:, 0])\n",
    "    plt.axis(\"equal\")\n",
    "    plt.title(\"X Coord Layla (Loss {:.3f})\".format(loss_xy_layla))\n",
    "    plt.subplot(338)\n",
    "    plt.scatter(YLayla.detach().cpu()[0, :, 2], YOut[:, 2])\n",
    "    plt.axis(\"equal\")\n",
    "    plt.title(\"R Coord Layla (Loss {:.3f})\".format(loss_rgb_layla))\n",
    "    \n",
    "    plt.subplot(339)\n",
    "    textstr = \"Epoch {}\\n\\nlam_xy = {}\\nlam_rgb = {}\\npre_scale={}\\nn_units={}\\nn_taps={}\\npre_scale={}\".format(epoch, lam_xy, lam_rgb, pre_scale, n_units, n_taps, pre_scale)\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax = plt.gca()\n",
    "    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.savefig(\"Epoch{}.png\".format(epoch), bbox_inches='tight')\n",
    "    \n",
    "    x = XN.detach().cpu().numpy()[0, :]\n",
    "    x = x/np.max(x)\n",
    "    x = np.array(x*32768, dtype=np.int16)\n",
    "    wavfile.write(\"Epoch{}.wav\".format(epoch), sr, x)\n",
    "    \n",
    "    x = N.detach().cpu().numpy()[0, :]\n",
    "    x = x/np.max(x)\n",
    "    x = np.array(x*32768, dtype=np.int16)\n",
    "    wavfile.write(\"Epoch{}Noise.wav\".format(epoch), sr, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c463f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
