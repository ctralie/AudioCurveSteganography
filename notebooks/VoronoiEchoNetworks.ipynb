{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_audiomentations import Compose, PitchShift\n",
    "from scipy import signal\n",
    "import glob\n",
    "import librosa\n",
    "import time\n",
    "import pickle\n",
    "import subprocess\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import skimage.io\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from imtools import get_voronoi_image, splat_voronoi_image_1nn\n",
    "from audioutils import extract_loudness, get_mp3_noise, get_batch_stft_noise\n",
    "from tsp import get_tsp_tour\n",
    "sys.path.append(\"../models\")\n",
    "from networks import CurveSTFTEncoder, CurveDecoder, BinaryDecoder\n",
    "from losses import mss_loss\n",
    "sys.path.append(\"../data\")\n",
    "from dataset import CurveData, AudioData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d73838",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb47884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print(\"Device: \", device)\n",
    "\n",
    "n_samples = 3000\n",
    "T = 300\n",
    "samples_per_batch = 10000\n",
    "win_length = 1024\n",
    "hop_length = win_length//2\n",
    "sr = 44100\n",
    "\n",
    "## Setup hann window for encoder\n",
    "hann = torch.hann_window(win_length).to(device)\n",
    "\n",
    "curve_train = CurveData(np.arange(10), n_samples, T, samples_per_batch)\n",
    "curve_test  = CurveData(np.arange(10, 15), n_samples, T, samples_per_batch)\n",
    "\n",
    "audio_train = AudioData(\"../data/musdb18hq/train/*/mixture.wav\", T, samples_per_batch, win_length, sr)\n",
    "audio_test  = AudioData(\"../data/musdb18hq/test/*/mixture.wav\",  T, samples_per_batch, win_length, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5adf2b",
   "metadata": {},
   "source": [
    "# Test Example - Layla And Smooth Criminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48484c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 3000\n",
    "n_iters = 100\n",
    "I = skimage.io.imread(\"../data/images/layla.png\")\n",
    "N = min(I.shape[0], I.shape[1])\n",
    "J, YLayla, final_cost = get_voronoi_image(I, 'cpu', n_points, n_neighbs=2, n_iters=n_iters, verbose=False, plot_iter_interval=0, use_lsqr=False)\n",
    "YLayla = get_tsp_tour(YLayla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9750be",
   "metadata": {},
   "outputs": [],
   "source": [
    "YLayla = np.array(YLayla, dtype=np.float32)\n",
    "TLayla = YLayla.shape[0]\n",
    "YLayla = torch.from_numpy(YLayla).to(device)\n",
    "YLayla = YLayla.view(1, TLayla, 5)\n",
    "xsmooth, _ = librosa.load(\"../data/tunes/smoothcriminal.mp3\", sr=sr)\n",
    "#xsmooth, _ = librosa.load(\"../data/musdb18hq/test/Arise - Run Run Run/mixture.wav\", sr=sr)\n",
    "xsmooth = xsmooth[sr*15::] # Cut off quieter beginning\n",
    "lsmooth = extract_loudness(xsmooth, sr, hop_length, win_length)\n",
    "lsmooth = np.array(lsmooth[0:TLayla], dtype=np.float32)\n",
    "xsmooth = np.array(xsmooth[0:hop_length*(TLayla-1)+win_length], dtype=np.float32)\n",
    "xsmooth = torch.from_numpy(xsmooth[None, :]).to(device)\n",
    "lsmooth = torch.from_numpy(lsmooth[None, :, None]).to(device)\n",
    "print(xsmooth.shape, lsmooth.shape)\n",
    "plt.imshow(splat_voronoi_image_1nn(YLayla.detach().cpu()[0, :, :], 512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7deb662",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters for encoder/decoder\n",
    "mlp_depth = 3\n",
    "n_units = 512\n",
    "noise_eps = 0.2\n",
    "use_mp3_noise = True\n",
    "n_taps = 10\n",
    "max_lag = 512\n",
    "tap_amp = 0.02\n",
    "tap_sigma = 5\n",
    "\n",
    "lam_xy = 150 # Weight for geometry curve fit\n",
    "lam_rgb = 50 # Weight for rgb curve fit\n",
    "lam_taps = 1 # Weight for change of taps\n",
    "\n",
    "encoder = CurveSTFTEncoder(mlp_depth, n_units, win_length, n_taps, max_lag, tap_amp, tap_sigma, 5)\n",
    "encoder = encoder.to(device)\n",
    "decoder = CurveDecoder(mlp_depth, n_units, win_length, 5)\n",
    "decoder = decoder.to(device)\n",
    "print(\"Encoder params\", encoder.get_num_parameters())\n",
    "print(\"Decoder params\", decoder.get_num_parameters())\n",
    "\n",
    "## Setup the loss function\n",
    "batch_size=16\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)\n",
    "# Start at learning rate of 0.001, rate decay of 0.98 factor every 10,000 steps\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000/len(curve_train), gamma=0.98)\n",
    "\n",
    "n_epochs = 1000\n",
    "xy_losses = []\n",
    "rgb_losses = []\n",
    "taps_losses = []\n",
    "audio_losses = []\n",
    "\n",
    "test_taps_losses = []\n",
    "test_xy_losses = []\n",
    "test_rgb_losses = []\n",
    "test_audio_losses = []\n",
    "\n",
    "## Setup data augmentation\n",
    "pitch_shifter = PitchShift(sample_rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fbcf50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fac = 0.7\n",
    "plt.figure(figsize=(fac*18, fac*24))\n",
    "\n",
    "epoch = 0\n",
    "while epoch < n_epochs:\n",
    "    #####################    STEP 1:  TRAIN     ##################### \n",
    "    curve_loader = DataLoader(curve_train, batch_size=16, shuffle=True)\n",
    "    audio_loader = DataLoader(audio_train, batch_size=16, shuffle=True)\n",
    "    \n",
    "    train_xy_loss = 0\n",
    "    train_rgb_loss = 0\n",
    "    train_audio_loss = 0\n",
    "    train_taps_loss = 0\n",
    "    tic = time.time()\n",
    "    n_per_batch = len(audio_loader)\n",
    "    for batch_num, (Y, (X, L)) in enumerate(zip(curve_loader, audio_loader)): # Go through each mini batch\n",
    "        # Reset the optimizer's gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move inputs/outputs to GPU\n",
    "        Y = Y.to(device)\n",
    "        X = X.to(device)\n",
    "        \n",
    "        # Do data augmentation on the audio samples X\n",
    "        X = pitch_shifter(X.view(X.shape[0], 1, X.shape[1]))[:, 0, :]\n",
    "        \n",
    "        # Encode the curve\n",
    "        N, taps = encoder(X, Y)        \n",
    "        \n",
    "        # Add filtered noise to audio and decode\n",
    "        XN = N\n",
    "        if noise_eps == 0 or np.random.rand() < 0.5:\n",
    "            YOut = decoder(XN)\n",
    "        else:\n",
    "            if use_mp3_noise:\n",
    "                added_noise = get_mp3_noise(XN, sr)\n",
    "            else:\n",
    "                added_noise = torch.randn(XN.shape).to(XN)\n",
    "            YOut = decoder(XN + noise_eps*added_noise)\n",
    "        \n",
    "        # Loss terms\n",
    "        loss_audio = mss_loss(X, XN)\n",
    "        \n",
    "        loss_xy  = lam_xy *torch.mean(torch.abs(YOut[:, :, 0:2]-Y[:, :, 0:2]))\n",
    "        loss_rgb = lam_rgb*torch.mean(torch.abs(YOut[:, :, 2::]-Y[:, :, 2::]))\n",
    "        \n",
    "        ## Minimize the amount by which the taps change from step to step\n",
    "        diff = taps[:, 1::, :] - taps[:, 0:-1, :]\n",
    "        diff = torch.sum(torch.abs(diff), dim=2)\n",
    "        loss_taps = lam_taps*torch.mean(diff)\n",
    "        \n",
    "        loss = loss_xy + loss_rgb + loss_audio #+ loss_taps\n",
    "        train_xy_loss += loss_xy.item()\n",
    "        train_rgb_loss += loss_rgb.item()\n",
    "        train_audio_loss += loss_audio.item()\n",
    "        train_taps_loss += loss_taps.item()\n",
    "        \n",
    "        # Compute the gradients of the loss function with respect\n",
    "        # to all of the parameters of the model\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        dt = time.time()-tic\n",
    "        rate = dt/(batch_num+1)\n",
    "        time_left = (rate*n_per_batch - dt)\n",
    "        minutes = int(np.floor(time_left/60))\n",
    "        seconds = int(time_left-60*minutes)\n",
    "        ipd.clear_output()\n",
    "        print(\"Epoch {}, batch {} of {}, {}m{}s left\".format(epoch, batch_num+1, n_per_batch, minutes, seconds), flush=True)\n",
    "        \n",
    "    last_Y = Y\n",
    "    last_YOut = YOut\n",
    "        \n",
    "    audio_losses.append(train_audio_loss/len(audio_loader))\n",
    "    xy_losses.append(train_xy_loss/len(audio_loader))\n",
    "    rgb_losses.append(train_rgb_loss/len(audio_loader))\n",
    "    taps_losses.append(train_taps_loss/len(audio_loader))\n",
    "    \n",
    "    print(\"Epoch {}, audio loss {:.3f}, xy loss {:.3f}, rgb loss {:.3f}\".format(epoch, audio_losses[-1], xy_losses[-1], rgb_losses[-1]))\n",
    "    scheduler.step()\n",
    "    \n",
    "    #####################    STEP 2:  TEST     ##################### \n",
    "    curve_loader = DataLoader(curve_test, batch_size=16)\n",
    "    audio_loader = DataLoader(audio_test, batch_size=16)\n",
    "    test_xy_loss = 0\n",
    "    test_rgb_loss = 0\n",
    "    test_audio_loss = 0\n",
    "    test_taps_loss = 0\n",
    "    for batch_num, (Y, (X, L)) in enumerate(zip(curve_loader, audio_loader)): # Go through each mini batch\n",
    "        # Move inputs/outputs to GPU\n",
    "        Y = Y.to(device)\n",
    "        X = X.to(device)\n",
    "        L = L.to(device)\n",
    "        # Encode the curve\n",
    "        N, taps_diff = encoder(X, Y)  \n",
    "        # Add filtered noise to audio and decode\n",
    "        XN = N\n",
    "        if not use_mp3_noise:\n",
    "            added_noise = torch.randn(XN.shape).to(XN)\n",
    "            YOut = decoder(XN + noise_eps*added_noise)\n",
    "        else:\n",
    "            # Skip adding mp3 noise for speed\n",
    "            YOut = decoder(XN)\n",
    "        # Loss terms\n",
    "        loss_audio = mss_loss(X, XN)\n",
    "        \n",
    "        loss_xy  = lam_xy *torch.mean(torch.abs(YOut[:, :, 0:2]-Y[:, :, 0:2]))\n",
    "        loss_rgb = lam_rgb*torch.mean(torch.abs(YOut[:, :, 2::]-Y[:, :, 2::]))\n",
    "        \n",
    "        # Minimize the amount by which the taps change from step to step\n",
    "        diff = taps[:, 1::, :] - taps[:, 0:-1, :]\n",
    "        diff = torch.sum(torch.abs(diff), dim=2)\n",
    "        loss_taps = lam_taps*torch.mean(diff)\n",
    "        \n",
    "        test_xy_loss += loss_xy.item()\n",
    "        test_rgb_loss += loss_rgb.item()\n",
    "        test_audio_loss += loss_audio.item()\n",
    "        test_taps_loss += loss_taps.item()\n",
    "        \n",
    "    last_Y_test = Y\n",
    "    last_YOut_test = YOut\n",
    "        \n",
    "    test_audio_losses.append(test_audio_loss/len(audio_loader))\n",
    "    test_xy_losses.append(test_xy_loss/len(audio_loader))\n",
    "    test_rgb_losses.append(test_rgb_loss/len(audio_loader))\n",
    "    test_taps_losses.append(test_taps_loss/len(audio_loader))\n",
    "    \n",
    "    \n",
    "    #####################    STEP 3:  LAYLA     ##################### \n",
    "    N, taps = encoder(xsmooth, YLayla)\n",
    "    XN = N\n",
    "    added_noise = torch.randn(XN.shape).to(XN)\n",
    "    YOut = decoder(XN + noise_eps*added_noise)\n",
    "    loss_layla_xy  = torch.mean(torch.abs(YOut[:, :, 0:2]-YLayla[:, :, 0:2]))\n",
    "    loss_layla_rgb = torch.mean(torch.abs(YOut[:, :, 2::]-YLayla[:, :, 2::]))\n",
    "\n",
    "    YOut = YOut.detach().cpu()[0, :, :].numpy()\n",
    "    # Save as an mp3 and repeat\n",
    "    x = XN.detach().cpu().numpy()[0, :]\n",
    "    x = x/np.max(x)\n",
    "    x = np.array(x*32768, dtype=np.int16)\n",
    "    wavfile.write(\"Epoch{}.wav\".format(epoch), sr, x)\n",
    "\n",
    "    x = N.detach().cpu().numpy()[0, :]\n",
    "    x = x/np.max(x)\n",
    "    x = np.array(x*32768, dtype=np.int16)\n",
    "    wavfile.write(\"Epoch{}Noise.wav\".format(epoch), sr, x)\n",
    "\n",
    "    subprocess.call(\"ffmpeg -i Epoch{}.wav Epoch{}.mp3\".format(epoch, epoch).split(), stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "    z, sr = librosa.load(\"Epoch{}.mp3\".format(epoch), sr=sr)\n",
    "    z = torch.from_numpy(z[None, :]).to(XN)\n",
    "    YOut_mp3 = decoder(z)\n",
    "    loss_layla_xy_mp3  = torch.mean(torch.abs(YOut_mp3[:, :, 0:2]-YLayla[:, :, 0:2]))\n",
    "    loss_layla_rgb_mp3 = torch.mean(torch.abs(YOut_mp3[:, :, 2::]-YLayla[:, :, 2::]))\n",
    "    YOut_mp3 = YOut_mp3.detach().cpu()[0, :, :].numpy()\n",
    "\n",
    "\n",
    "\n",
    "    #####################    STEP 4: PLOT    ##################### \n",
    "    plt.clf()\n",
    "    plt.subplot(431)\n",
    "    plt.imshow(splat_voronoi_image_1nn(YOut, 512, 512))\n",
    "    plt.title(\"Layla Wav Reconstruction\")\n",
    "    plt.subplot(432)\n",
    "    plt.imshow(splat_voronoi_image_1nn(YOut_mp3, 512, 512))\n",
    "    plt.title(\"Layla Mp3 Reconstruction\")\n",
    "\n",
    "    plt.subplot(433)\n",
    "    plt.plot(audio_losses)\n",
    "    plt.plot(xy_losses)\n",
    "    plt.plot(rgb_losses)\n",
    "    plt.plot(taps_losses)\n",
    "    plt.legend([\"Audio ({:.3f})\".format(audio_losses[-1]), \n",
    "                \"xy ({:.3f})\".format(xy_losses[-1]),\n",
    "                \"rgb ({:.3f})\".format(rgb_losses[-1]),\n",
    "                \"taps ({:.3f})\".format(taps_losses[-1])])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Scaled Loss\")\n",
    "    plt.title(\"Epoch {}, Train Losses\".format(epoch)) \n",
    "\n",
    "\n",
    "    plt.subplot(434)\n",
    "    plt.scatter(YLayla.detach().cpu()[0, :, 0], YOut[:, 0])\n",
    "    plt.axis(\"equal\")\n",
    "    plt.title(\"Wav X Coord Layla (Mean Error {:.3f})\".format(loss_layla_xy))\n",
    "    plt.subplot(435)\n",
    "    plt.scatter(YLayla.detach().cpu()[0, :, 2], YOut[:, 2])\n",
    "    plt.axis(\"equal\")\n",
    "    plt.title(\"Wav R Coord Layla (Mean Error {:.3f})\".format(loss_layla_rgb))\n",
    "\n",
    "    plt.subplot(436)\n",
    "    plt.plot(test_audio_losses)\n",
    "    plt.plot(test_xy_losses)\n",
    "    plt.plot(test_rgb_losses)\n",
    "    plt.plot(test_taps_losses)\n",
    "    plt.legend([\"Audio ({:.3f})\".format(test_audio_losses[-1]), \n",
    "                \"xy ({:.3f})\".format(test_xy_losses[-1]), \n",
    "                \"rgb ({:.3f})\".format(test_rgb_losses[-1]), \n",
    "                \"taps ({:.3f})\".format(test_taps_losses[-1])])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Scaled Loss\")\n",
    "    plt.title(\"Test Losses\")\n",
    "\n",
    "    plt.subplot(437)\n",
    "    plt.scatter(YLayla.detach().cpu()[0, :, 0], YOut_mp3[:, 0])\n",
    "    plt.axis(\"equal\")\n",
    "    plt.title(\"Mp3 X Coord Layla (Mean Error {:.3f})\".format(loss_layla_xy_mp3))\n",
    "    plt.subplot(438)\n",
    "    plt.scatter(YLayla.detach().cpu()[0, :, 2], YOut_mp3[:, 2])\n",
    "    plt.axis(\"equal\")\n",
    "    plt.title(\"Mp3 R Coord Layla (Mean Error {:.3f})\".format(loss_layla_rgb_mp3))\n",
    "\n",
    "    plt.subplot(439)\n",
    "    textstr = \"Epoch {}\\n\\nlam_xy = {}\\nlam_rgb = {}\\nlam_taps = {}\\nn_units={}\\nn_taps={}\\nmax_lag={}\\ntap_amp={}\\ntap_sigma={}\".format(epoch, lam_xy, lam_rgb, lam_taps, n_units, n_taps, max_lag, tap_amp, tap_sigma)\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax = plt.gca()\n",
    "    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot2grid((4, 3), (3, 0))\n",
    "    plt.imshow(taps.detach().cpu()[0, :, 1::], aspect='auto', interpolation='none', cmap='magma_r')\n",
    "    plt.subplot2grid((4, 3), (3, 1), colspan=2)\n",
    "    plt.plot(taps.detach().cpu()[0, 0, 1::])\n",
    "    plt.plot(taps.detach().cpu()[0, 2, 1::])\n",
    "    plt.plot(taps.detach().cpu()[0, 3, 1::])\n",
    "\n",
    "    plt.savefig(\"Epoch{}.png\".format(epoch), bbox_inches='tight')\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), \"encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(), \"decoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9074f1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
